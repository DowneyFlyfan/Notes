# 常用的模型初始化方法及其实现
*   初始化方法

1.  Xavier / Glorot 初始化  
    (\begin{gather} \begin{aligned} Xavier(x) & \sim U( - \sqrt{ \dfrac{ 6 }{ F_{in} + F_{out} } }, \sqrt{ \dfrac{ 6 }{ F_{in} + F_{out} } }) \\\ Glorot(x) &\sim N(0, \sqrt{ \dfrac{ 2 }{ F_{in} + F_{out} } }) \end{aligned} \end{gather}$  
      
    适用于 Sigmoid, tanh **零中心的** 激活函数网络  
      
    (F_{in}, F_{out}$指的是输入、输出节点数，即该层输出的特征维度（全连接层）或通道数（卷积层）。  
      
    
2.   何恺明正态分布初始化  
    \\[KM(x) \sim N(0, \sqrt{ \dfrac{ 2 }{ F_{in} }})]适用于ReLu和类ReLu激活函数网络, 能够起到防止梯度消失/爆炸的作用  
      
    
3.  Yann Lecun初始化  
    \\[LeCun(x) \sim N(0, \sqrt{ \dfrac{ 1 }{ F_{in} } })]适用于Sigmoid, tanh 等 **零中心的** 激活函数网络  
      
    
4.  常数初始化
5.  正交初始化
6.  正态分布初始化
7.  均匀分布初始化
8.  两端截断的正态分布初始化
